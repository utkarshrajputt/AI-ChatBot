{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3z9/go2nJzaUytDLY/L3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshrajputt/AI-ChatBot/blob/Utkarsh/CHATBOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL3U-YWm9Uug",
        "outputId": "8b4c8f4c-9fcb-40f8-cf2f-c7b0c988d4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m248.7/248.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq langchain wget llama-index cohere llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "def bar_custom(current, total, width=80):\n",
        "    print(\"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total))\n",
        "\n",
        "model_url=\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q2_K.gguf\"\n",
        "wget.download(model_url, bar=bar_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "m_8qORAB9zgN",
        "outputId": "d68f5db1-fd2d-4741-ea26-86ba7b46daba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 0% [0 / 65876] bytes\n",
            "Downloading: 12% [8192 / 65876] bytes\n",
            "Downloading: 24% [16384 / 65876] bytes\n",
            "Downloading: 37% [24576 / 65876] bytes\n",
            "Downloading: 49% [32768 / 65876] bytes\n",
            "Downloading: 62% [40960 / 65876] bytes\n",
            "Downloading: 74% [49152 / 65876] bytes\n",
            "Downloading: 87% [57344 / 65876] bytes\n",
            "Downloading: 99% [65536 / 65876] bytes\n",
            "Downloading: 100% [65876 / 65876] bytes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'llama-2-7b-chat.Q2_K (1).gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install streamlit"
      ],
      "metadata": {
        "id": "s9FWTm0N9zju"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from llama_index import(\n",
        "    SimpleDirectoryReader,\n",
        "   VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt\n",
        ")\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "def init_page() -> None:\n",
        "  st.set_page_config(\n",
        "      page_title=\"Personal ChatBot\",\n",
        "      page_icon=\"ðŸ¦™\",\n",
        "      layout=\"wide\",\n",
        "  )\n",
        "  st.header(\"ðŸ¦™ Personal ChatBot\")\n",
        "  st.sidebar.title(\"Options\")\n",
        "\n",
        "def select_llm() -> LlamaCPP:\n",
        "  return LlamaCPP(\n",
        "      model_path=\"llama-2-7b-chat.Q2_K.gguf\",\n",
        "      temperature=0.1,\n",
        "      max_new_tokens=500,\n",
        "      context_window=3900,\n",
        "      generate_kwargs={},\n",
        "      model_kwargs={\"n_gpu_layers\": 1},\n",
        "      message_to_prompt=messages_to_prompt,\n",
        "      completion_to_prompt=completion_to_prompt,\n",
        "      verbose=True,\n",
        "  )\n",
        "\n",
        "  def init_messages() -> None:\n",
        "    clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
        "    if clear_button or \"messages\" not in st.session_state:\n",
        "      st.session_state.messages = [\n",
        "        SystemMessage(\n",
        "          content=\"you are a helpful AI assistant. Reply your answer in markdown format.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def get_answer(llm, messages) -> str:\n",
        "  response = llm.complete(messages)\n",
        "  return response.text\n",
        "\n",
        "def main() -> None:\n",
        "  init_page()\n",
        "  llm = select_llm()\n",
        "  init_messages()\n",
        "\n",
        "  if user_input := st.chat_input(\"Input your question!\"):\n",
        "    st.session_state.messages.append(HumanMessage(content=user_input))\n",
        "    with st.spinner(\"Bot is typing ...\"):\n",
        "      answer = get_answer(llm, user_input)\n",
        "      print(answer)\n",
        "    st.session_state.messages.append(AIMessage(content=answer))\n",
        "\n",
        "\n",
        "  messages = st.session_state.get(\"messages\", [])\n",
        "  for message in messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "      with st.chat_message(\"assistant\"):\n",
        "        st.markdown(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "      with st.chat_message(\"user\"):\n",
        "        st.markdown(message.content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmf_atlm_DfZ",
        "outputId": "49580100-1c34-4676-8392-21373f2c5e71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZtk5zrnREj8",
        "outputId": "2e050542-6bf1-4ac7-f776-f37b0665180e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "  localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.179.56:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}